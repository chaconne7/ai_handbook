# Techniques {#techniques}

## Overview

Machine learning can be broadly bucketed into three categories:

(1) Supervised Learning
(2) Unsupervised Learning
(3) Reinforcement Learning

Supervised learning encompasses tasks that try to predict something, such as stock price, or whether a picture contains a dog or a cat.  To train a supervised ML model, you must define both input features and outputs, so we call this labeled data.

Unsupervised learning aims to identify structure in unlabeled data, meaning you provide only input features.  Examples include clustering books by topic, identifying when a machine might be faulty due to odd behavior, etc.

Reinforcement learning trains an agent to accomplish a complex goal by identifying the next best step to take.  Examples include playing PacMan or chess.

In each of these categories, an engineer must specify an objective function to maximize or minimize, and define how to incorporate feedback from a correct or incorrect decision back into a model to adjust its parameters.

## Supervised Learning

Within supervised learning, there are typically two tasks we're interested in:

(1) Regression - predicting a continous value, such as housing price
(2) Classification

Useful Resources

- [Classification Versus Regression](https://medium.com/simple-ai/classification-versus-regression-intro-to-machine-learning-5-5566efd4cb83)

### Regression

#### Linear Regression

#### ARIMA

Useful Resources

- [Introduction to Forecasting with ARIMA in R](https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials)
- [Step By Step Graphic Guide to Forecasting Through ARIMA Modeling In R](http://ucanalytics.com/blogs/step-by-step-graphic-guide-to-forecasting-through-arima-modeling-in-r-manufacturing-case-study-example/)
- [ARIMA Models - Manufacturing Case Study Example](http://ucanalytics.com/blogs/arima-models-manufacturing-case-study-example-part-3/)
- [](https://otexts.org/fpp2/arima.html)
- [Time Series Seasonal ARIMA Model In Python](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/)

#### Prophet

Useful Resources

- [Prophet Quickstart](https://facebook.github.io/prophet/docs/quick_start.html#python-api)

### Classification

#### K-Nearest Neighbors (KNN)

#### Logistic Regression

#### Naive Bayes

#### Support Vector Machines (SVM)

#### Decision Trees

Useful Resources

- [Decision Trees - An Intuitive Introduction](https://medium.com/x8-the-ai-community/decision-trees-an-intuitive-introduction-86c2b39c1a6c)
- [How to Visualize a Decision Tree from a Random Forest in Python using Scikit-Learn](https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c)

#### Random Forests

Useful Resources

- [Machine Learning Crash Course: Part 5 - Decision Trees and Ensemble Models](https://ml.berkeley.edu/blog/2017/12/26/tutorial-5/)

### Neural Networks

#### Convolutional Neural Networks (CNN)

![](img/convnet.jpg)

Useful Resources

- [Intro to Convolutional Neural Networks](http://cs231n.github.io/convolutional-networks/)
- [Visualizing what ConvNets Learn](http://cs231n.github.io/understanding-cnn/)

#### Recurrent Neural Networks (RNN)

#### Long Short Term Memory (LSTM)

Useful Resources

- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## Unsupervised Learning

### Clustering

#### K-Means

![](img/kmeans.gif)

K-means clustering is one of the most common ways to identify groups in your data.  You start with unlabeled data (i.e. online customer sales data) and want to identify what clusters they can be bucketed into (i.e. customer archetypes).

How does it work?  You define an objective function to minimize, initialize the number and location of cluster centroids (randomly or through a heuristic), and keep repeating the following steps until some criteria is met (i.e. 1000 iterations or objective function is sufficiently small):

1. Update the cluster assignments of each data point
    - Each data point is assigned to the cluster centroid that is closest
2. Update the cluster centroids
    - Each cluster center is the average of the data points assigned to it

Note that k-means clustering usually requires some manual investigation.  The best way to start is to visualize your data and see if you can roughly identify groups.  If you see three groups, then start by initializing three cluster centroids.  If you can’t identify any groupings visually, then it’s a pretty good bet that whatever clustering the algorithm assigns isn’t going to be useful or interpretable.

Useful Resources:

- [K-Means Clustering: From A to Z (September 2018, Towards Data Science)](https://towardsdatascience.com/k-means-clustering-from-a-to-z-f6242a314e9a)

#### Gaussian Mixture Model

### Dimensionality Reduction

#### PCA

- [Visualising high-dimensional datasets using PCA and t-SNE in Python (October 2016, Medium)](https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)

#### t-SNE

- [Visualising high-dimensional datasets using PCA and t-SNE in Python (October 2016, Medium)](https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)

### Neural Networks

#### Autoencoders

![](img/autoencoder.png)

Autoencoders are neural networks that try to learn a latent representation of some input, which is a fancy way of saying it figures out the important features that capture the essence of the input.  The purpose of building an autoencoder is typically:

- Dimensionality reduction or encoding data
    - Example: “Visualizing high-dimensional data is challenging. t-SNE is the most commonly used method but struggles with large number of dimensions (typically above 32). So autoencoders are used as a preprocessing step to reduce the dimensionality, and this compressed representation is used by t-SNE to visualize the data in 2D space.” ^[https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798]
- Denoise data
    - Example: You have a grainy image but the random pixel values that cause the “graininess” is not useful for classifying what the image is.  You can adjust how you use the autoencoder by feeding it the grainy image and having it try to predict the clean image.

Autoencoders are comprised of two parts - an encoder neural network and a decoder neural network - and the inputs and outputs are the same.  When the input is fed through the encoder, it is forced into a smaller, “lossy” representation due to the smaller number of neurons at the end of the encoder network.  This compressed representation is then fed through the decoder, which tries to reconstruct the input.  During the training process, the autoencoder therefore learns to approximate an identity function, and is forced to extract the most relevant features of the input.

For the more technical folks, the function of an autoencoder is similar to principal component analysis (PCA).

Useful Resources:

- [How Autoencoders Work - Understanding the Math and Implementation (Kaggle)](https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases)
- [Deep inside: Autoencoders (February 2018, Towards Data Science)](https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f)
- [Autoencoders - Deep Learning Bits #1 (February 2017, Hacker Noon)](https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694)
- [Applied Deep Learning - Part 3: Autoencoders (October 2017, Towards Data Science)](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)

#### Generative Adversarial Networks

## Reinforcement Learning
