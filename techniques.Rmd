# Techniques {#techniques}

## Overview

Machine learning can be broadly bucketed into three categories:

(1) Supervised Learning
(2) Unsupervised Learning
(3) Reinforcement Learning

Supervised learning encompasses tasks that try to predict something, such as stock price, or whether a picture contains a dog or a cat.  To train a supervised ML model, you must define both input features and outputs, so we call this labeled data.

Unsupervised learning aims to identify structure in unlabeled data, meaning you provide only input features.  Examples include clustering books by topic, identifying when a machine might be faulty due to odd behavior, etc.

Reinforcement learning trains an agent to accomplish a complex goal by identifying the next best step to take.  Examples include playing PacMan or chess.

In each of these categories, an engineer must specify an objective function to maximize or minimize, and define how to incorporate feedback from a correct or incorrect decision back into a model to adjust its parameters.

## Supervised Learning

Within supervised learning, there are typically two tasks we're interested in:

(1) Regression - predicting a continous value, such as housing price
(2) Classification

Useful Resources

- [Classification Versus Regression](https://medium.com/simple-ai/classification-versus-regression-intro-to-machine-learning-5-5566efd4cb83)

### Regression

#### Linear Regression

#### ARIMA

Useful Resources

- [Introduction to Forecasting with ARIMA in R](https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials)
- [Step By Step Graphic Guide to Forecasting Through ARIMA Modeling In R](http://ucanalytics.com/blogs/step-by-step-graphic-guide-to-forecasting-through-arima-modeling-in-r-manufacturing-case-study-example/)
- [ARIMA Models - Manufacturing Case Study Example](http://ucanalytics.com/blogs/arima-models-manufacturing-case-study-example-part-3/)
- [](https://otexts.org/fpp2/arima.html)
- [Time Series Seasonal ARIMA Model In Python](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/)

#### Prophet

Useful Resources

- [Prophet Quickstart](https://facebook.github.io/prophet/docs/quick_start.html#python-api)

### Classification

#### K-Nearest Neighbors (KNN)

#### Logistic Regression

#### Naive Bayes

#### Support Vector Machines (SVM)

#### Decision Trees

Useful Resources

- [Decision Trees - An Intuitive Introduction](https://medium.com/x8-the-ai-community/decision-trees-an-intuitive-introduction-86c2b39c1a6c)
- [How to Visualize a Decision Tree from a Random Forest in Python using Scikit-Learn](https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c)

#### Random Forests

Useful Resources

- [Machine Learning Crash Course: Part 5 - Decision Trees and Ensemble Models](https://ml.berkeley.edu/blog/2017/12/26/tutorial-5/)

### Neural Networks

#### Convolutional Neural Networks (CNN)

![](img/convnet.jpg)

Useful Resources

- [Intro to Convolutional Neural Networks](http://cs231n.github.io/convolutional-networks/)
- [Visualizing what ConvNets Learn](http://cs231n.github.io/understanding-cnn/)

#### Recurrent Neural Networks (RNN)

![](img/rnn.png)

When we process sequence data, like reading a paragraph or listening to music, we often rely on the context of what we've processed before to make decisions about the future.  A basic neural network doesn't accomplish this - it can only make decisions based on the current input.  Fortunately, recurrent neural networks (RNNs) come to the rescue!

An RNN is a neural network that generates not only an output, but also a "hidden state" that feeds back into itself.  While this may seem complex, we can unroll this network in time to see how the hidden state feeds into the next timestep along with the next timestep's inputs.  This hidden state represents the context of previous inputs, so we aren't throwing away previous learnings!

In each timestep, we therefore incorporate the previous timestep's context (or nothing at the start), along with the current input, to generate an output (i.e. the next word in a sentence translation).

Useful Resources:

- [The Unreasonable Effectiveness of Recurrent Neural Networks (May 2015, Andrej Karpathy)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

#### Long Short Term Memory (LSTM)

![](img/lstm.png)

One challenge with RNNs is that they have trouble retaining long-term information.  For instance, consider what Christopher Olah (colah) has to say:

"If we are trying to predict the last word in 'the clouds are in the sky,' we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information."

On the other hand, "consider trying to predict the last word in the text 'I grew up in France… I speak fluent French.' Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large."

Due to a phenomenon known as the vanishing gradient problem (see Advanced Concepts section), an RNN's capability to learn from information from many timesteps ago quickly diminishes.  Fortunately, we have Long Short Term Memory networks (LSTMs) to the rescue!

Here's the basic concept of an LSTM:

- Prior information is recorded in a cell state that is passed along at each timestep
- At each timestep, based on the previous output and the current input, the network contains three gates that control whether or not information can pass through:
    - "Forget gate": How much of the previous cell state to forget (between 0 and 1)
    - "Input gate": How much of the current information to incorporate into the new cell state
    - "Output gate": What to output from the new cell state

The practical effect of these gates is that we can now ignore "useless" new information and keep its noise from overwhelming the actual signal from many timesteps ago.  LSTMs and its variants, such as the Gated Recurrent Unit (GRU) have proven themselves to be far superior to RNNs.

Useful Resources:

- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## Unsupervised Learning

### Clustering

#### K-Means

![](img/kmeans.gif)

K-means clustering is one of the most common ways to identify groups in your data.  You start with unlabeled data (i.e. online customer sales data) and want to identify what clusters they can be bucketed into (i.e. customer archetypes).

How does it work?  You define an objective function to minimize, initialize the number and location of cluster centroids (randomly or through a heuristic), and keep repeating the following steps until some criteria is met (i.e. 1000 iterations or objective function is sufficiently small):

1. Update the cluster assignments of each data point
    - Each data point is assigned to the cluster centroid that is closest
2. Update the cluster centroids
    - Each cluster center is the average of the data points assigned to it

Note that k-means clustering usually requires some manual investigation.  The best way to start is to visualize your data and see if you can roughly identify groups.  If you see three groups, then start by initializing three cluster centroids.  If you can’t identify any groupings visually, then it’s a pretty good bet that whatever clustering the algorithm assigns isn’t going to be useful or interpretable.

Useful Resources:

- [K-Means Clustering: From A to Z (September 2018, Towards Data Science)](https://towardsdatascience.com/k-means-clustering-from-a-to-z-f6242a314e9a)

#### Gaussian Mixture Model

### Dimensionality Reduction

#### PCA

- [Visualising high-dimensional datasets using PCA and t-SNE in Python (October 2016, Medium)](https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)

#### t-SNE

- [Visualising high-dimensional datasets using PCA and t-SNE in Python (October 2016, Medium)](https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)

### Neural Networks

#### Autoencoders

![](img/autoencoder.png)

Autoencoders are neural networks that try to learn a latent representation of some input, which is a fancy way of saying it figures out the important features that capture the essence of the input.  The purpose of building an autoencoder is typically:

- Dimensionality reduction or encoding data
    - Example: “Visualizing high-dimensional data is challenging. t-SNE is the most commonly used method but struggles with large number of dimensions (typically above 32). So autoencoders are used as a preprocessing step to reduce the dimensionality, and this compressed representation is used by t-SNE to visualize the data in 2D space.” ^[https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798]
- Denoise data
    - Example: You have a grainy image but the random pixel values that cause the “graininess” is not useful for classifying what the image is.  You can adjust how you use the autoencoder by feeding it the grainy image and having it try to predict the clean image.

Autoencoders are comprised of two parts - an encoder neural network and a decoder neural network - and the inputs and outputs are the same.  When the input is fed through the encoder, it is forced into a smaller, “lossy” representation due to the smaller number of neurons at the end of the encoder network.  This compressed representation is then fed through the decoder, which tries to reconstruct the input.  During the training process, the autoencoder therefore learns to approximate an identity function, and is forced to extract the most relevant features of the input.

For the more technical folks, the function of an autoencoder is similar to principal component analysis (PCA).

Useful Resources:

- [How Autoencoders Work - Understanding the Math and Implementation (Kaggle)](https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases)
- [Deep inside: Autoencoders (February 2018, Towards Data Science)](https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f)
- [Autoencoders - Deep Learning Bits #1 (February 2017, Hacker Noon)](https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694)
- [Applied Deep Learning - Part 3: Autoencoders (October 2017, Towards Data Science)](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)

#### Generative Adversarial Networks

## Reinforcement Learning
